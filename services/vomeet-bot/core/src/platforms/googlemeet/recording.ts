import type { Page } from "playwright";
import { WhisperLiveService } from "../../services/whisperlive";
import type { BotConfig } from "../../types";
import { log } from "../../utils";
import { ensureBrowserUtils } from "../../utils/injection";
import {
	googleNameSelectors,
	googleParticipantContainerSelectors,
	googleParticipantSelectors,
	googlePeopleButtonSelectors,
	googleSilenceClassNames,
	googleSpeakingClassNames,
	googleSpeakingIndicators,
} from "./selectors";

// Modified to use new services - Google Meet recording functionality
export async function startGoogleRecording(
	page: Page,
	botConfig: BotConfig,
): Promise<void> {
	// Initialize WhisperLive service on Node.js side
	const whisperLiveService = new WhisperLiveService({
		whisperLiveUrl: process.env.WHISPER_LIVE_URL,
	});

	// Initialize WhisperLive connection with STUBBORN reconnection - NEVER GIVES UP!
	const whisperLiveUrl =
		await whisperLiveService.initializeWithStubbornReconnection("Google Meet");

	log(`[Node.js] Using WhisperLive URL for Google Meet: ${whisperLiveUrl}`);
	log("Starting Google Meet recording with WebSocket connection");

	await ensureBrowserUtils(
		page,
		require("node:path").join(__dirname, "../../browser-utils.global.js"),
	);

	// Pass the necessary config fields and the resolved URL into the page context
	await page.evaluate(
		async (pageArgs: {
			botConfigData: BotConfig;
			whisperUrlForBrowser: string;
			selectors: {
				participantSelectors: string[];
				speakingClasses: string[];
				silenceClasses: string[];
				containerSelectors: string[];
				nameSelectors: string[];
				speakingIndicators: string[];
				peopleButtonSelectors: string[];
			};
		}) => {
			const { botConfigData, whisperUrlForBrowser, selectors } = pageArgs;

			// Use browser utility classes from the global bundle
			const browserUtils = (window as any).VomeetBrowserUtils;
			(window as any).logBot(
				`Browser utils available: ${Object.keys(browserUtils || {}).join(", ")}`,
			);

			// --- Early reconfigure wiring (stub + event) ---
			// Queue reconfig requests until service is ready
			(window as any).__vomeetPendingReconfigure = null;
			if (typeof (window as any).triggerWebSocketReconfigure !== "function") {
				(window as any).triggerWebSocketReconfigure = async (
					lang: string | null,
					task: string | null,
				) => {
					(window as any).__vomeetPendingReconfigure = { lang, task };
					(window as any).logBot?.(
						"[Reconfigure] Stub queued update; will apply when service is ready.",
					);
				};
			}
			try {
				document.addEventListener("vomeet:reconfigure", (ev: Event) => {
					try {
						const detail = (ev as CustomEvent).detail || {};
						const { lang, task } = detail;
						const fn = (window as any).triggerWebSocketReconfigure;
						if (typeof fn === "function") fn(lang, task);
					} catch {}
				});
			} catch {}
			// ---------------------------------------------

			const audioService = new browserUtils.BrowserAudioService({
				targetSampleRate: 16000,
				bufferSize: 4096,
				inputChannels: 1,
				outputChannels: 1,
			});

			// Use BrowserWhisperLiveService with stubborn mode to enable reconnection on Google Meet
			const whisperLiveService = new browserUtils.BrowserWhisperLiveService(
				{
					whisperLiveUrl: whisperUrlForBrowser,
				},
				true,
			); // Enable stubborn mode for Google Meet

			// Expose references for reconfiguration
			(window as any).__vomeetWhisperLiveService = whisperLiveService;
			(window as any).__vomeetBotConfig = botConfigData;

			// Replace stub with real reconfigure implementation and apply any queued update
			(window as any).triggerWebSocketReconfigure = async (
				lang: string | null,
				task: string | null,
			) => {
				try {
					const svc = (window as any).__vomeetWhisperLiveService;
					const cfg = (window as any).__vomeetBotConfig || {};
					cfg.language = lang;
					cfg.task = task || "transcribe";
					(window as any).__vomeetBotConfig = cfg;
					try {
						svc?.close();
					} catch {}
					await svc?.connectToWhisperLive(
						cfg,
						(window as any).__vomeetOnMessage,
						(window as any).__vomeetOnError,
						(window as any).__vomeetOnClose,
					);
					(window as any).logBot?.(
						`[Reconfigure] Applied: language=${cfg.language}, task=${cfg.task}`,
					);
				} catch (e: any) {
					(window as any).logBot?.(
						`[Reconfigure] Error applying new config: ${e?.message || e}`,
					);
				}
			};
			try {
				const pending = (window as any).__vomeetPendingReconfigure;
				if (
					pending &&
					typeof (window as any).triggerWebSocketReconfigure === "function"
				) {
					(window as any).triggerWebSocketReconfigure(
						pending.lang,
						pending.task,
					);
					(window as any).__vomeetPendingReconfigure = null;
				}
			} catch {}

			await new Promise<void>((resolve, reject) => {
				try {
					(window as any).logBot(
						"Starting Google Meet recording process with new services.",
					);

					// Find and create combined audio stream
					audioService
						.findMediaElements()
						.then(async (mediaElements: HTMLMediaElement[]) => {
							if (mediaElements.length === 0) {
								reject(
									new Error(
										"[Google Meet BOT Error] No active media elements found after multiple retries. Ensure the Google Meet meeting media is playing.",
									),
								);
								return;
							}

							// Create combined audio stream
							return await audioService.createCombinedAudioStream(
								mediaElements,
							);
						})
						.then(async (combinedStream: MediaStream | undefined) => {
							if (!combinedStream) {
								reject(
									new Error(
										"[Google Meet BOT Error] Failed to create combined audio stream",
									),
								);
								return;
							}
							// Initialize audio processor
							return await audioService.initializeAudioProcessor(
								combinedStream,
							);
						})
						.then(async (_processor: any) => {
							// Setup audio data processing
							audioService.setupAudioDataProcessor(
								async (
									audioData: Float32Array,
									_sessionStartTime: number | null,
								) => {
									// Only send after server ready (canonical Teams pattern)
									if (!whisperLiveService.isReady()) {
										// Skip sending until server is ready
										return;
									}
									// Compute simple RMS and peak for diagnostics
									let sumSquares = 0;
									let peak = 0;
									for (let i = 0; i < audioData.length; i++) {
										const v = audioData[i];
										sumSquares += v * v;
										const a = Math.abs(v);
										if (a > peak) peak = a;
									}
									const _rms = Math.sqrt(
										sumSquares / Math.max(1, audioData.length),
									);
									// Diagnostic: send metadata first
									whisperLiveService.sendAudioChunkMetadata(
										audioData.length,
										16000,
									);
									// Send audio data to WhisperLive
									const success = whisperLiveService.sendAudioData(audioData);
									if (!success) {
										(window as any).logBot(
											"Failed to send Google Meet audio data to WhisperLive",
										);
									}
								},
							);

							// Initialize WhisperLive WebSocket connection with simple reconnection wrapper
							const connectWhisper = async () => {
								try {
									// Define callbacks so they can be reused for reconfiguration reconnects
									const onMessage = (data: any) => {
										const logFn = (window as any).logBot;
										// Reduce log spam: log only important status changes and completed transcript segments
										if (!data || typeof data !== "object") {
											return;
										}
										if (data.status === "ERROR") {
											logFn(
												`Google Meet WebSocket Server Error: ${data.message}`,
											);
											return;
										}
										if (data.status === "WAIT") {
											logFn(`Google Meet Server busy: ${data.message}`);
											return;
										}
										if (
											!whisperLiveService.isReady() &&
											data.status === "SERVER_READY"
										) {
											whisperLiveService.setServerReady(true);
											logFn("Google Meet Server is ready.");
											return;
										}
										if (data.language) {
											if (!(window as any).__vomeetLangLogged) {
												(window as any).__vomeetLangLogged = true;
												logFn(
													`Google Meet Language detected: ${data.language}`,
												);
											}
											// do not return; language can accompany segments
										}
										if (data.message === "DISCONNECT") {
											logFn("Google Meet Server requested disconnect.");
											whisperLiveService.close();
											return;
										}
										// Log only completed transcript segments, with deduplication
										if (Array.isArray(data.segments)) {
											const completedTexts = data.segments
												.filter((s: any) => s?.completed && s.text)
												.map((s: any) => s.text as string);
											if (completedTexts.length > 0) {
												const transcriptKey = completedTexts.join(" ").trim();
												if (
													transcriptKey &&
													transcriptKey !== (window as any).__lastTranscript
												) {
													(window as any).__lastTranscript = transcriptKey;
													logFn(`Transcript: ${transcriptKey}`);
												}
											}
										}
									};
									const onError = (_event: Event) => {
										(window as any).logBot(
											`[Google Meet Failover] WebSocket error. This will trigger retry logic.`,
										);
									};
									const onClose = async (event: CloseEvent) => {
										(window as any).logBot(
											`[Google Meet Failover] WebSocket connection closed. Code: ${event.code}, Reason: ${event.reason}. Attempting reconnect in 2s...`,
										);
										try {
											whisperLiveService.setServerReady(false);
										} catch {}
										setTimeout(() => {
											// Best-effort reconnect; BrowserWhisperLiveService stubborn mode should also help
											connectWhisper().catch(() => {});
										}, 2000);
									};

									// Save callbacks globally for reuse
									(window as any).__vomeetOnMessage = onMessage;
									(window as any).__vomeetOnError = onError;
									(window as any).__vomeetOnClose = onClose;

									await whisperLiveService.connectToWhisperLive(
										(window as any).__vomeetBotConfig,
										onMessage,
										onError,
										onClose,
									);
								} catch (e) {
									(window as any).logBot(
										`Google Meet connect error: ${(e as any)?.message || e}. Retrying in 2s...`,
									);
									setTimeout(() => {
										connectWhisper().catch(() => {});
									}, 2000);
								}
							};
							return await connectWhisper();
						})
						.then(() => {
							// Initialize Google-specific speaker detection (Teams-style with Google selectors)
							(window as any).logBot(
								"Initializing Google Meet speaker detection...",
							);

							const initializeGoogleSpeakerDetection = (
								whisperLiveService: any,
								audioService: any,
								botConfigData: any,
							) => {
								const selectorsTyped = selectors as any;

								const speakingStates = new Map<string, string>();

								function getGoogleParticipantId(element: HTMLElement) {
									let id = element.getAttribute("data-participant-id");
									if (!id) {
										const stableChild = element.querySelector(
											"[jsinstance]",
										) as HTMLElement | null;
										if (stableChild) {
											id =
												stableChild.getAttribute("jsinstance") ||
												(undefined as any);
										}
									}
									if (!id) {
										if (!(element as any).dataset.vomeetGeneratedId) {
											(element as any).dataset.vomeetGeneratedId =
												`gm-id-${Math.random().toString(36).substr(2, 9)}`;
										}
										id = (element as any).dataset.vomeetGeneratedId;
									}
									return id as string;
								}

								function getGoogleParticipantName(
									participantElement: HTMLElement,
								) {
									// Prefer explicit Meet name spans
									const notranslate = participantElement.querySelector(
										"span.notranslate",
									) as HTMLElement | null;
									if (notranslate?.textContent?.trim()) {
										const t = notranslate.textContent.trim();
										if (t.length > 1 && t.length < 50) return t;
									}

									// Try configured name selectors
									const nameSelectors: string[] =
										selectorsTyped.nameSelectors || [];
									for (const sel of nameSelectors) {
										const el = participantElement.querySelector(
											sel,
										) as HTMLElement | null;
										if (el) {
											let nameText =
												el.textContent ||
												el.innerText ||
												el.getAttribute("data-self-name") ||
												el.getAttribute("aria-label") ||
												"";
											if (nameText) {
												nameText = nameText.trim();
												if (
													nameText &&
													nameText.length > 1 &&
													nameText.length < 50
												)
													return nameText;
											}
										}
									}

									// Fallbacks
									const selfName =
										participantElement.getAttribute("data-self-name");
									if (selfName?.trim()) return selfName.trim();
									const idToDisplay =
										getGoogleParticipantId(participantElement);
									return `Google Participant (${idToDisplay})`;
								}

								function isVisible(el: HTMLElement): boolean {
									const cs = getComputedStyle(el);
									const rect = el.getBoundingClientRect();
									const ariaHidden = el.getAttribute("aria-hidden") === "true";
									return (
										rect.width > 0 &&
										rect.height > 0 &&
										cs.display !== "none" &&
										cs.visibility !== "hidden" &&
										cs.opacity !== "0" &&
										!ariaHidden
									);
								}

								function hasSpeakingIndicator(container: HTMLElement): boolean {
									const indicators: string[] =
										selectorsTyped.speakingIndicators || [];
									for (const sel of indicators) {
										const ind = container.querySelector(
											sel,
										) as HTMLElement | null;
										if (ind && isVisible(ind)) return true;
									}
									return false;
								}

								function inferSpeakingFromClasses(
									container: HTMLElement,
									mutatedClassList?: DOMTokenList,
								): { speaking: boolean } {
									const speakingClasses: string[] =
										selectorsTyped.speakingClasses || [];
									const silenceClasses: string[] =
										selectorsTyped.silenceClasses || [];

									const classList = mutatedClassList || container.classList;
									const descendantSpeaking = speakingClasses.some((cls) =>
										container.querySelector(`.${cls}`),
									);
									const hasSpeaking =
										speakingClasses.some((cls) => classList.contains(cls)) ||
										descendantSpeaking;
									const hasSilent = silenceClasses.some((cls) =>
										classList.contains(cls),
									);
									if (hasSpeaking) return { speaking: true };
									if (hasSilent) return { speaking: false };
									return { speaking: false };
								}

								function sendGoogleSpeakerEvent(
									eventType: string,
									participantElement: HTMLElement,
								) {
									const sessionStartTime =
										audioService.getSessionAudioStartTime();
									if (sessionStartTime === null) {
										return;
									}
									const relativeTimestampMs = Date.now() - sessionStartTime;
									const participantId =
										getGoogleParticipantId(participantElement);
									const participantName =
										getGoogleParticipantName(participantElement);
									try {
										whisperLiveService.sendSpeakerEvent(
											eventType,
											participantName,
											participantId,
											relativeTimestampMs,
											botConfigData,
										);
									} catch {}
								}

								function logGoogleSpeakerEvent(
									participantElement: HTMLElement,
									mutatedClassList?: DOMTokenList,
								) {
									const participantId =
										getGoogleParticipantId(participantElement);
									const participantName =
										getGoogleParticipantName(participantElement);
									const previousLogicalState =
										speakingStates.get(participantId) || "silent";

									// Primary: indicators; Fallback: classes
									const indicatorSpeaking =
										hasSpeakingIndicator(participantElement);
									const classInference = inferSpeakingFromClasses(
										participantElement,
										mutatedClassList,
									);
									const isCurrentlySpeaking =
										indicatorSpeaking || classInference.speaking;

									if (isCurrentlySpeaking) {
										if (previousLogicalState !== "speaking") {
											(window as any).logBot(
												`ðŸŽ¤ [Google] SPEAKER_START: ${participantName} (ID: ${participantId})`,
											);
											sendGoogleSpeakerEvent(
												"SPEAKER_START",
												participantElement,
											);
										}
										speakingStates.set(participantId, "speaking");
									} else {
										if (previousLogicalState === "speaking") {
											(window as any).logBot(
												`ðŸ”‡ [Google] SPEAKER_END: ${participantName} (ID: ${participantId})`,
											);
											sendGoogleSpeakerEvent("SPEAKER_END", participantElement);
										}
										speakingStates.set(participantId, "silent");
									}
								}

								function observeGoogleParticipant(
									participantElement: HTMLElement,
								) {
									const participantId =
										getGoogleParticipantId(participantElement);
									speakingStates.set(participantId, "silent");

									// Initial scan
									logGoogleSpeakerEvent(participantElement);

									const callback = (mutationsList: MutationRecord[]) => {
										for (const mutation of mutationsList) {
											if (
												mutation.type === "attributes" &&
												mutation.attributeName === "class"
											) {
												const targetElement = mutation.target as HTMLElement;
												if (
													participantElement.contains(targetElement) ||
													participantElement === targetElement
												) {
													logGoogleSpeakerEvent(
														participantElement,
														targetElement.classList,
													);
												}
											}
										}
									};

									const observer = new MutationObserver(callback);
									observer.observe(participantElement, {
										attributes: true,
										attributeFilter: ["class"],
										subtree: true,
									});

									if (
										!(participantElement as any).dataset.vomeetObserverAttached
									) {
										(participantElement as any).dataset.vomeetObserverAttached =
											"true";
									}
								}

								function scanForAllGoogleParticipants() {
									const participantSelectors: string[] =
										selectorsTyped.participantSelectors || [];
									for (const sel of participantSelectors) {
										document.querySelectorAll(sel).forEach((el) => {
											const elh = el as HTMLElement;
											if (!(elh as any).dataset.vomeetObserverAttached) {
												observeGoogleParticipant(elh);
											}
										});
									}
								}

								// Attempt to click People button to stabilize DOM if available
								try {
									const peopleSelectors: string[] =
										selectorsTyped.peopleButtonSelectors || [];
									for (const sel of peopleSelectors) {
										const btn = document.querySelector(
											sel,
										) as HTMLElement | null;
										if (btn && isVisible(btn)) {
											btn.click();
											break;
										}
									}
								} catch {}

								// Initialize
								scanForAllGoogleParticipants();

								// Polling fallback to catch speaking indicators not driven by class mutations
								const lastSpeakingById = new Map<string, boolean>();
								setInterval(() => {
									const participantSelectors: string[] =
										selectorsTyped.participantSelectors || [];
									const elements: HTMLElement[] = [];
								participantSelectors.forEach((sel) => {
									document
										.querySelectorAll(sel)
										.forEach((el) => {
											elements.push(el as HTMLElement);
										});
								});
									elements.forEach((container) => {
										const id = getGoogleParticipantId(container);
										const indicatorSpeaking =
											hasSpeakingIndicator(container) ||
											inferSpeakingFromClasses(container).speaking;
										const prev = lastSpeakingById.get(id) || false;
										if (indicatorSpeaking && !prev) {
											(window as any).logBot(
												`[Google Poll] SPEAKER_START ${getGoogleParticipantName(container)}`,
											);
											sendGoogleSpeakerEvent("SPEAKER_START", container);
											lastSpeakingById.set(id, true);
											speakingStates.set(id, "speaking");
										} else if (!indicatorSpeaking && prev) {
											(window as any).logBot(
												`[Google Poll] SPEAKER_END ${getGoogleParticipantName(container)}`,
											);
											sendGoogleSpeakerEvent("SPEAKER_END", container);
											lastSpeakingById.set(id, false);
											speakingStates.set(id, "silent");
										} else if (!lastSpeakingById.has(id)) {
											lastSpeakingById.set(id, indicatorSpeaking);
										}
									});
								}, 500);
							};

							initializeGoogleSpeakerDetection(
								whisperLiveService,
								audioService,
								botConfigData,
							);

							// Simple single-strategy participant extraction from main video area
							(window as any).logBot(
								"Initializing participant counting via video tiles and participant panel...",
							);

							const extractParticipantsFromMain = (
								botName: string | undefined,
							): string[] => {
								const participants: string[] = [];
								
								// Method 1: Count video tiles with participant names
								// Google Meet shows participant names on video tiles
								const videoTiles = document.querySelectorAll('[data-self-name], [data-participant-id]');
								videoTiles.forEach((tile) => {
									const name = tile.getAttribute('data-self-name');
									if (name && name.trim()) {
										participants.push(name.trim());
									}
								});
								
								// Method 2: Look for participant name spans in the meeting view
								// These are typically in spans with class containing participant info
								const nameSpans = document.querySelectorAll('div[data-participant-id] span.notranslate, [data-self-name] span.notranslate');
								nameSpans.forEach((span) => {
									const text = (span.textContent || '').trim();
									if (text && text.length > 1 && text.length < 50) {
										participants.push(text);
									}
								});
								
								// Method 3: Check the "In this call" panel if open
								// Look for participant list items
								const participantListItems = document.querySelectorAll('[role="listitem"] span.notranslate');
								participantListItems.forEach((item) => {
									const text = (item.textContent || '').trim();
									if (text && text.length > 1 && text.length < 50) {
										participants.push(text);
									}
								});
								
								// Method 4: Look for video container labels/tooltips
								const tooltips = document.querySelectorAll('[role="tooltip"]');
								tooltips.forEach((el: Element) => {
									const text = (el.textContent || "").trim();
									if (text && text.length > 1 && text.length < 50) {
										participants.push(text);
									}
								});
								
								// Method 5: Look for the participant count indicator (e.g., "2" next to people icon)
								// This is a fallback to get at least the count
								const peopleCountBadge = document.querySelector('[data-tab-id="1"] .google-material-icons + span, [aria-label*="participant"] span');
								if (peopleCountBadge) {
									const countText = (peopleCountBadge.textContent || '').trim();
									const count = parseInt(countText, 10);
									if (!isNaN(count) && count > 0) {
										// If we found a count badge but no names, add placeholder entries
										if (participants.length === 0) {
											for (let i = 0; i < count; i++) {
												participants.push(`Participant ${i + 1}`);
											}
										}
									}
								}
								
								// Filter out the bot itself if we know its name
								const uniqueParticipants = Array.from(new Set(participants));
								if (botName) {
									return uniqueParticipants.filter(p => p !== botName);
								}
								return uniqueParticipants;
							};

							(window as any).getGoogleMeetActiveParticipants = () => {
								const names = extractParticipantsFromMain(
									(botConfigData as any)?.botName,
								);
								(window as any).logBot(
									`ðŸ” [Google Meet Participants] ${JSON.stringify(names)}`,
								);
								return names;
							};
							
							// More reliable count that uses multiple methods
							(window as any).getGoogleMeetActiveParticipantsCount = () => {
								// Method 1: Try to get count from the participant badge in toolbar
								// Look for the people button with a count
								const peopleButtons = document.querySelectorAll('[data-tab-id="1"], [aria-label*="people"], [aria-label*="participant"]');
								for (const btn of peopleButtons) {
									// Look for a number badge or text near the button
									const badge = btn.querySelector('span');
									if (badge) {
										const countText = (badge.textContent || '').trim();
										const count = parseInt(countText, 10);
										if (!isNaN(count) && count > 0) {
											(window as any).logBot(`[Participant Count] Badge method: ${count}`);
											return count;
										}
									}
								}
								
								// Method 2: Count video tiles (usually one per participant)
								const videoContainers = document.querySelectorAll('[data-participant-id], [data-self-name]');
								if (videoContainers.length > 0) {
									(window as any).logBot(`[Participant Count] Video tiles method: ${videoContainers.length}`);
									return videoContainers.length;
								}
								
								// Method 3: Fall back to name extraction
								const names = (window as any).getGoogleMeetActiveParticipants();
								(window as any).logBot(`[Participant Count] Name extraction method: ${names.length}`);
								return names.length;
							};

							// Setup Google Meet meeting monitoring (browser context)
							const setupGoogleMeetingMonitoring = (
								botConfigData: any,
								audioService: any,
								whisperLiveService: any,
								resolve: any,
							) => {
								(window as any).logBot(
									"Setting up Google Meet meeting monitoring...",
								);

								const leaveCfg =
									(botConfigData && (botConfigData as any).automaticLeave) ||
									{};
								const startupAloneTimeoutSeconds = Number(
									(leaveCfg.noOneJoinedTimeout ?? 20 * 60 * 1000) / 1000,
								);
								const everyoneLeftTimeoutSeconds = Number(
									(leaveCfg.everyoneLeftTimeout ?? 2 * 60 * 1000) / 1000,
								);

								let aloneTime = 0;
								let lastParticipantCount = 0;
								let speakersIdentified = false;
								let hasEverHadMultipleParticipants = false;

								const checkInterval = setInterval(() => {
									// Check participant count using the comprehensive helper
									const currentParticipantCount = (window as any)
										.getGoogleMeetActiveParticipantsCount
										? (window as any).getGoogleMeetActiveParticipantsCount()
										: 0;

									if (currentParticipantCount !== lastParticipantCount) {
										(window as any).logBot(
											`Participant check: Found ${currentParticipantCount} unique participants from central list.`,
										);
										lastParticipantCount = currentParticipantCount;

										// Track if we've ever had multiple participants
										if (currentParticipantCount > 1) {
											hasEverHadMultipleParticipants = true;
											speakersIdentified = true; // Once we see multiple participants, we've identified speakers
											(window as any).logBot(
												"Speakers identified - switching to post-speaker monitoring mode",
											);
										}
									}

									if (currentParticipantCount <= 1) {
										aloneTime++;

										// Determine timeout based on whether speakers have been identified
										const currentTimeout = speakersIdentified
											? everyoneLeftTimeoutSeconds
											: startupAloneTimeoutSeconds;
										const timeoutDescription = speakersIdentified
											? "post-speaker"
											: "startup";

										if (aloneTime >= currentTimeout) {
											if (speakersIdentified) {
												(window as any).logBot(
													`Google Meet meeting ended or bot has been alone for ${everyoneLeftTimeoutSeconds} seconds after speakers were identified. Stopping recorder...`,
												);
												clearInterval(checkInterval);
												audioService.disconnect();
												whisperLiveService.close();
												reject(new Error("GOOGLE_MEET_BOT_LEFT_ALONE_TIMEOUT"));
											} else {
												(window as any).logBot(
													`Google Meet bot has been alone for ${startupAloneTimeoutSeconds / 60} minutes during startup with no other participants. Stopping recorder...`,
												);
												clearInterval(checkInterval);
												audioService.disconnect();
												whisperLiveService.close();
												reject(
													new Error("GOOGLE_MEET_BOT_STARTUP_ALONE_TIMEOUT"),
												);
											}
										} else if (aloneTime > 0 && aloneTime % 10 === 0) {
											// Log every 10 seconds to avoid spam
											if (speakersIdentified) {
												(window as any).logBot(
													`Bot has been alone for ${aloneTime} seconds (${timeoutDescription} mode). Will leave in ${currentTimeout - aloneTime} more seconds.`,
												);
											} else {
												const remainingMinutes = Math.floor(
													(currentTimeout - aloneTime) / 60,
												);
												const remainingSeconds =
													(currentTimeout - aloneTime) % 60;
												(window as any).logBot(
													`Bot has been alone for ${aloneTime} seconds during startup. Will leave in ${remainingMinutes}m ${remainingSeconds}s.`,
												);
											}
										}
									} else {
										aloneTime = 0; // Reset if others are present
										if (hasEverHadMultipleParticipants && !speakersIdentified) {
											speakersIdentified = true;
											(window as any).logBot(
												"Speakers identified - switching to post-speaker monitoring mode",
											);
										}
									}
								}, 1000);

								// Listen for page unload
								window.addEventListener("beforeunload", () => {
									(window as any).logBot(
										"Page is unloading. Stopping recorder...",
									);
									clearInterval(checkInterval);
									audioService.disconnect();
									whisperLiveService.close();
									resolve();
								});

								document.addEventListener("visibilitychange", () => {
									if (document.visibilityState === "hidden") {
										(window as any).logBot(
											"Document is hidden. Stopping recorder...",
										);
										clearInterval(checkInterval);
										audioService.disconnect();
										whisperLiveService.close();
										resolve();
									}
								});
							};

							setupGoogleMeetingMonitoring(
								botConfigData,
								audioService,
								whisperLiveService,
								resolve,
							);
						})
						.catch((err: any) => {
							reject(err);
						});
				} catch (error: any) {
					return reject(new Error(`[Google Meet BOT Error] ${error.message}`));
				}
			});

			// Define reconfiguration hook to update language/task and reconnect
			(window as any).triggerWebSocketReconfigure = async (
				lang: string | null,
				task: string | null,
			) => {
				try {
					const svc = (window as any).__vomeetWhisperLiveService;
					const cfg = (window as any).__vomeetBotConfig || {};
					if (!svc) {
						(window as any).logBot?.(
							"[Reconfigure] WhisperLive service not initialized.",
						);
						return;
					}
					cfg.language = lang;
					cfg.task = task || "transcribe";
					(window as any).__vomeetBotConfig = cfg;
					try {
						svc.close();
					} catch {}
					await svc.connectToWhisperLive(
						cfg,
						(window as any).__vomeetOnMessage,
						(window as any).__vomeetOnError,
						(window as any).__vomeetOnClose,
					);
					(window as any).logBot?.(
						`[Reconfigure] Applied: language=${cfg.language}, task=${cfg.task}`,
					);
				} catch (e: any) {
					(window as any).logBot?.(
						`[Reconfigure] Error applying new config: ${e?.message || e}`,
					);
				}
			};
		},
		{
			botConfigData: botConfig,
			whisperUrlForBrowser: whisperLiveUrl,
			selectors: {
				participantSelectors: googleParticipantSelectors,
				speakingClasses: googleSpeakingClassNames,
				silenceClasses: googleSilenceClassNames,
				containerSelectors: googleParticipantContainerSelectors,
				nameSelectors: googleNameSelectors,
				speakingIndicators: googleSpeakingIndicators,
				peopleButtonSelectors: googlePeopleButtonSelectors,
			} as any,
		},
	);

	// After page.evaluate finishes, cleanup services
	await whisperLiveService.cleanup();
}
